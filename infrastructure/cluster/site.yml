---
# Professional MLOps Platform Deployment

# Play 1a: K3s Control Plane
- name: Setup K3s Control Plane
  hosts: k3s_control_plane
  become: true
  gather_facts: true
  tasks:
    - name: Debug before calling control plane role
      ansible.builtin.debug:
        msg: "About to call k3s_control_plane role with k3s_state={{ k3s_state | default('undefined') }}"
      tags: [k3s, control_plane, debug]

    - name: Deploy or Remove K3s Control Plane
      include_role:
        name: k3s_control_plane
      tags: [k3s, control_plane]

# Play 1b: K3s Workers
- name: Setup K3s Workers
  hosts: k3s_workers
  become: true
  gather_facts: true
  tasks:
    - name: Deploy or Remove K3s Workers
      include_role:
        name: k3s_workers
      tags: [k3s, workers]

# Play 2: NFS Infrastructure Setup
- name: Setup NFS Infrastructure
  hosts: nfs_server:k3s_control_plane:k3s_workers
  gather_facts: true
  become: true
  tasks:
    # NFS Server
    - name: Deploy NFS Server
      include_role:
        name: nfs_server
      when: 
        - inventory_hostname in groups['nfs_server']
        - k3s_state is not defined or k3s_state != "absent"
      tags: [storage, nfs, nfs-server]

    # NFS Clients
    - name: Deploy NFS Clients
      include_role:
        name: nfs_clients
      when: 
        - inventory_hostname in groups['k3s_control_plane'] or inventory_hostname in groups['k3s_workers']
        - k3s_state is not defined or k3s_state != "absent"
      tags: [storage, nfs, nfs-clients]

# Play 3: Kubernetes Infrastructure Setup
- name: Setup Kubernetes Infrastructure
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Deploy Kubernetes Components
      block:
        # Kubeconfig
        - name: Fetch kubeconfig
          include_role:
            name: fetch_kubeconfig
          tags: [k3s, kubeconfig]

        # NFS Provisioner
        - name: Deploy NFS Provisioner
          include_role:
            name: nfs_provisioner
          tags: [storage, nfs, nfs-provisioner]

        # Sealed Secrets
        - name: Deploy Sealed Secrets
          include_role:
            name: sealed_secrets
          tags: [security, sealed-secrets]

      when: k3s_state is not defined or k3s_state != "absent"

# Play 4: Core Platform Deployment (All Helm charts)
- name: Deploy Core Platform
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Deploy All Core Components
      block:
        # Helm repositories
        - name: Add Helm repositories
          kubernetes.core.helm_repository:
            name: "{{ item.name }}"
            repo_url: "{{ item.url }}"
            kubeconfig: "{{ kubeconfig_path }}"
          loop:
            - { name: "prometheus-community", url: "https://prometheus-community.github.io/helm-charts" }
            - { name: "grafana", url: "https://grafana.github.io/helm-charts" }
            - { name: "bitnami", url: "https://charts.bitnami.com/bitnami" }
            - { name: "argo", url: "https://argoproj.github.io/argo-helm" }
            - { name: "seldon", url: "https://storage.googleapis.com/seldon-charts" }
            - { name: "kubernetes-dashboard", url: "https://kubernetes.github.io/dashboard/" }
            - { name: "jupyterhub", url: "https://hub.jupyter.org/helm-chart/" }
          retries: 3  # Keep retry logic
          delay: 10   # Keep delay between retries
          tags: [core, helm-repos]

        # MinIO
        - name: Deploy MinIO (S3-compatible storage)
          kubernetes.core.helm:
            name: minio
            chart_ref: bitnami/minio
            release_namespace: minio
            create_namespace: true
            kubeconfig: "{{ kubeconfig_path }}"
            wait: true
            wait_timeout: 600s
            values:
              mode: "{{ minio_mode | default('standalone') }}"
              replicas: "{{ minio_replicas | default(1) }}"
              persistence:
                enabled: true
                size: "{{ minio_storage_size }}"
                storageClass: "{{ global_storage_class | default('nfs-shared') }}"
              auth:
                rootUser: "{{ minio_access_key }}"
                rootPassword: "{{ minio_secret_key }}"
              service:
                type: NodePort
                nodePorts:
                  api: "{{ minio_nodeport | int }}"
              consoleService:
                type: NodePort
                nodePorts:
                  console: "{{ minio_console_nodeport | int }}"
              resources:
                requests:
                  memory: "{{ minio_memory_request }}"
                  cpu: "{{ minio_cpu_request }}"
                limits:
                  memory: "{{ minio_memory_limit }}"
                  cpu: "{{ minio_cpu_limit }}"
          tags: [core, storage, minio]

        # Prometheus Stack (Corrected)
        - name: Deploy Prometheus Stack
          kubernetes.core.helm:
            name: prometheus-stack
            chart_ref: prometheus-community/kube-prometheus-stack
            release_namespace: monitoring
            create_namespace: true
            kubeconfig: "{{ kubeconfig_path }}"
            wait: true
            wait_timeout: 600s
            values:
              grafana:
                service:
                  type: NodePort
                  nodePort: "{{ grafana_nodeport }}"
                adminPassword: "{{ grafana_admin_password }}"
              prometheus:
                service:
                  type: NodePort
                  nodePort: "{{ prometheus_nodeport }}"
                prometheusSpec:
                  storageSpec:
                    volumeClaimTemplate:
                      spec:
                        storageClassName: "{{ global_storage_class }}"
                        accessModes: ["ReadWriteOnce"]
                        resources:
                          requests:
                            storage: "{{ prometheus_storage_size }}"
          retries: 3
          delay: 30
          tags: [core, monitoring, prometheus]

        # Argo CD
        - name: Deploy Argo CD
          kubernetes.core.helm:
            name: argocd
            chart_ref: argo/argo-cd
            release_namespace: argocd
            create_namespace: true
            kubeconfig: "{{ kubeconfig_path }}"
            wait: true
            wait_timeout: 600s
            values:
              server:
                service:
                  type: NodePort
                  nodePort: "{{ argocd_nodeport | default(30080) }}"
          tags: [mlops, argocd]

        # Argo Workflows
        - name: Deploy Argo Workflows
          kubernetes.core.helm:
            name: argo-workflows
            chart_ref: argo/argo-workflows
            release_namespace: argowf
            create_namespace: true
            kubeconfig: "{{ kubeconfig_path }}"
            wait: true
            wait_timeout: 600s
            values:
              server:
                authModes: ["server"]
                serviceType: NodePort
                serviceNodePort: 32746
                extraArgs:
                  - --auth-mode=server
              controller:
                workflowNamespaces:
                  - argowf
                  - default
                resources:
                  requests:
                    memory: "{{ argowf_memory_request | default('256Mi') }}"
                    cpu: "{{ argowf_cpu_request | default('100m') }}"
                  limits:
                    memory: "{{ argowf_memory_limit | default('512Mi') }}"
                    cpu: "{{ argowf_cpu_limit | default('500m') }}"
          tags: [mlops, workflows]

        # Apply Argo Workflows sealed secret
        - name: Apply Argo Workflows Sealed Secret
          kubernetes.core.k8s:
            kubeconfig: "{{ kubeconfig_path }}"
            src: "{{ playbook_dir }}/manifests/argo-workflows-admin-sealed-secret.yaml"
            state: present
          tags: [mlops, workflows, sealed-secrets]

        # MLflow
        - name: Deploy MLflow
          include_role:
            name: mlflow
          tags: [mlops, mlflow]

        # Kubernetes Dashboard (Complete deployment)
        - name: Deploy Kubernetes Dashboard with Admin Access
          block:
            - name: Deploy Dashboard Helm Chart
              kubernetes.core.helm:
                name: kubernetes-dashboard
                chart_ref: kubernetes-dashboard/kubernetes-dashboard
                release_namespace: kubernetes-dashboard
                create_namespace: true
                kubeconfig: "{{ kubeconfig_path }}"
                wait: true
                wait_timeout: 600s
                values:
                  kong:
                    service:
                      type: NodePort
                      nodePort: 30444
                  app:
                    settings:
                      global:
                        skipLogin: true
                  extraArgs:
                    - --enable-skip-login
                    - --disable-settings-authorizer

            - name: Create Dashboard Admin ServiceAccount and ClusterRoleBinding
              kubernetes.core.k8s:
                kubeconfig: "{{ kubeconfig_path }}"
                definition: "{{ item }}"
              loop:
                - apiVersion: v1
                  kind: ServiceAccount
                  metadata:
                    name: dashboard-admin
                    namespace: kubernetes-dashboard
                - apiVersion: rbac.authorization.k8s.io/v1
                  kind: ClusterRoleBinding
                  metadata:
                    name: dashboard-admin
                  roleRef:
                    apiGroup: rbac.authorization.k8s.io
                    kind: ClusterRole
                    name: cluster-admin
                  subjects:
                  - kind: ServiceAccount
                    name: dashboard-admin
                    namespace: kubernetes-dashboard

            - name: Create NodePort Service for Dashboard Access
              kubernetes.core.k8s:
                kubeconfig: "{{ kubeconfig_path }}"
                definition:
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: dashboard-nodeport
                    namespace: kubernetes-dashboard
                  spec:
                    type: NodePort
                    ports:
                    - port: 443
                      targetPort: 8443
                      nodePort: 30444
                      protocol: TCP
                    selector:
                      app.kubernetes.io/name: kong

          tags: [core, dashboard]

        # Jupyter Hub (Corrected)
        - name: Deploy JupyterHub
          kubernetes.core.helm:
            name: jupyterhub
            chart_ref: jupyterhub/jupyterhub
            release_namespace: jupyterhub
            create_namespace: true
            kubeconfig: "{{ kubeconfig_path }}"
            wait: true
            wait_timeout: 600s
            values:
              proxy:
                service:
                  type: NodePort
                  nodePorts:
                    http: 30888  # Correct syntax for JupyterHub
              hub:
                config:
                  DummyAuthenticator:
                    password: "{{ jupyter_password | default('mlops123') }}"
                  JupyterHub:
                    authenticator_class: "dummy"
              singleuser:
                image:
                  name: jupyter/datascience-notebook
                  tag: "latest"
                defaultUrl: "/lab"
                storage:
                  type: none  # No persistent storage for demo
          tags: [mlops, jupyter]

        # Seldon Core (Model Serving Platform)
        - name: Deploy Seldon Core
          kubernetes.core.helm:
            name: seldon-core
            chart_ref: seldon/seldon-core-operator
            release_namespace: seldon-system
            create_namespace: true
            kubeconfig: "{{ kubeconfig_path }}"
            wait: true
            wait_timeout: 600s
            values:
              usageMetrics:
                enabled: true
              istio:
                enabled: false  # Use without Istio for simplicity
              ambassador:
                enabled: false  # Use without Ambassador
              certManager:
                enabled: false  # Use without cert-manager
              keda:
                enabled: false  # Disable auto-scaling for now
          tags: [mlops, seldon, model-serving]

        # Kubeflow Pipelines (via kubectl manifests - Fixed URL)
        - name: Deploy Kubeflow Pipelines via manifests
          kubernetes.core.k8s:
            kubeconfig: "{{ kubeconfig_path }}"
            state: present
            src: "https://raw.githubusercontent.com/kubeflow/pipelines/2.5.0/manifests/kustomize/env/platform-agnostic-multi-user/kustomization.yaml"
          ignore_errors: true  # Continue if this fails
          tags: [mlops, kubeflow]

# Play 4: Demo Application
- name: Deploy Demo Application
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Deploy ML Demo Pipeline
      block:
        - include_role:
            name: mlops_demo_app
      when: k3s_state is not defined or k3s_state != "absent"
      tags: [demo, mlops-demo]

# Play 5: Summary and Cleanup
- name: Deployment Summary and Cleanup
  hosts: localhost:nfs_server
  gather_facts: false
  tasks:
    # Deployment Summary (localhost only)
    - name: Show deployment summary
      debug:
        msg:
          - "ðŸŽ‰ Production MLOps Platform Deployment Complete!"
          - ""
          - "ðŸš€ MLOps Core Services:"
          - "- MLflow: http://192.168.1.85:30800"
          - "- Seldon Core: http://192.168.1.85:32000"
          - "- Argo CD: http://192.168.1.85:30080"
          - "- JupyterHub: http://192.168.1.85:30888"
          - "- Kubernetes Dashboard: http://192.168.1.85:30443"
          - ""
          - "ðŸ“Š Monitoring & Storage:"
          - "- Grafana: http://192.168.1.85:30300"
          - "- Prometheus: http://192.168.1.85:30090"
          - "- MinIO Console: http://192.168.1.85:30901"
          - ""
          - "ðŸ”§ Cluster Management:"
          - "- Kubeflow Pipelines: http://192.168.1.85:31234 (if deployed)"
          - "- Argo Workflows: http://192.168.1.85:31001 (if deployed)"
          - ""
          - "ðŸ”‘ Access Instructions:"
          - "- kubectl: export KUBECONFIG=/tmp/k3s-kubeconfig.yaml"
          - "- Dashboard: Use --enable-skip-login for demo access"
      when: 
        - inventory_hostname == 'localhost'
        - k3s_state is not defined or k3s_state != "absent"
      tags: [summary]

    # NFS Cleanup (nfs_server only)
    - name: Clean up NFS storage
      shell: |
        find /srv/nfs/kubernetes/ -mindepth 1 -maxdepth 1 -type d -exec rm -rf {} \;
        echo "Cleaned up NFS storage"
      become: true
      when: 
        - inventory_hostname in groups['nfs_server']
        - k3s_state is defined and k3s_state == "absent"
      tags: [cleanup, nfs-cleanup]