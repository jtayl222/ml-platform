---
# =============================================================================
# K3S CLUSTER CONFIGURATION
# High-Resource Cluster: 5 nodes, 36 CPU cores, ~250GB RAM
# =============================================================================

ansible_python_interpreter: /usr/bin/python3

# K3s and Kubeconfig settings
k3s_server_path: /etc/rancher/k3s/k3s.yaml

# =============================================================================
# CORE INFRASTRUCTURE
# =============================================================================

# K3s Configuration
k3s_version: v1.33.1+k3s1
k3s_state: present

# Kubeconfig Management
kubeconfig_dir: "{{ playbook_dir }}/../fetched_tokens"
kubeconfig_path: "{{ kubeconfig_dir }}/k3s-kubeconfig"
control_plane_host: "{{ groups['k3s_control_plane'][0] }}"
control_plane_ip: "{{ hostvars[groups['k3s_control_plane'][0]]['ansible_host'] }}"

# Network Configuration
k3s_pod_cidr: "10.42.0.0/16"
k3s_service_cidr: "10.43.0.0/16"
k3s_api_port: 6443

# CNI Configuration (Cilium default, resolves Calico ARP bug #8689)
cilium_enabled: true
calico_enabled: false

# Storage
global_storage_class: "nfs-shared"
nfs_server_ip: "{{ hostvars[groups['nfs_server'][0]]['ansible_host'] }}"
nfs_path: "/srv/nfs/kubernetes"
nfs_allowed_networks: "192.168.1.0/24"

# =============================================================================
# OPTIMIZED RESOURCE ALLOCATION FOR HIGH-RESOURCE ON-PREMISES DEPLOYMENT
# =============================================================================

# MinIO - High-Performance Storage
minio_namespace: "minio"
minio_nodeport: 30900
minio_console_nodeport: 30901
minio_memory_request: "8Gi"
minio_memory_limit: "16Gi"
minio_cpu_request: 4
minio_cpu_limit: 8
minio_storage_size: "1Ti"
minio_internal_endpoint: "http://minio.minio.svc.cluster.local:9000"

# Grafana - High-Performance Dashboards  
grafana_nodeport: 30300
grafana_admin_user: "admin"
grafana_admin_password: "admin123"
grafana_memory_request: "4Gi"
grafana_memory_limit: "8Gi"
grafana_cpu_request: 2
grafana_cpu_limit: 4
grafana_storage_size: "100Gi"

# MLflow - Enhanced ML Experiment Tracking
mlflow_namespace: "mlflow"
mlflow_nodeport: 30800
mlflow_image: "192.168.1.210/library/mlflow-postgresql:3.1.0-4"
mlflow_storage_size: "500Gi"
mlflow_memory_request: "8Gi"
mlflow_memory_limit: "16Gi"
mlflow_cpu_request: 4
mlflow_cpu_limit: 8
mlflow_s3_bucket: "mlflow-artifacts"
mlflow_s3_endpoint: "http://minio.minio.svc.cluster.local:9000"
mlflow_storage_class: "{{ global_storage_class }}"

# External PostgreSQL Configuration for MLflow
mlflow_db_enabled: true
mlflow_db_host: "192.168.1.100"
mlflow_db_port: "5432"  
mlflow_db_name: "mlflow"
mlflow_authdb_name: "authdb"

# MLflow Configuration (updated)
mlflow_backend_store_type: "postgresql"  # Changed from file-based
mlflow_enable_model_registry: true       # Enable Model Registry features

# Seldon Core - High-Performance Model Serving
seldon_namespace: "seldon-system"
seldon_chart_repo: "https://storage.googleapis.com/seldon-charts"
seldon_chart_repo_name: "seldon"
seldon_chart_name: "seldon-core-operator"
seldon_memory_request: "4Gi"
seldon_memory_limit: "12Gi"
seldon_cpu_request: 2
seldon_cpu_limit: 8
seldon_minio_bucket: "seldon-models"
seldon_enable_analytics: true

# Seldon Core v2 Custom Configuration
# Enable PR #6582 - SELDON_SERVER_HOST support
seldon_server_host:
  enabled: true
  value: "localhost"

# Enable custom images for PR testing
seldon_custom_images:
  enabled: true
  registry: "192.168.1.210"
  agent:
    repository: "library/seldon-agent"
    tag: "2.9.0-pr6582-test"
    pullPolicy: "Always"

# Additional agent environment variables for PR #6582
seldon_agent_env_vars:
  - name: "SELDON_LOG_LEVEL"
    value: "info"

# JupyterHub - High-Performance Data Science Environment
jupyterhub_namespace: "jupyterhub"
jupyterhub_nodeport: 30888
jupyterhub_password: "mlops123"
# Enhanced user resources for heavy ML workloads
jupyterhub_user_memory_request: "4G"
jupyterhub_user_memory_limit: "16G"
jupyterhub_user_cpu_request: 1.0
jupyterhub_user_cpu_limit: 8.0
jupyterhub_storage_type: "dynamic"
jupyterhub_storage_size: "50G"
jupyterhub_image_name: "192.168.1.210/library/financial-predictor-jupyter"
jupyterhub_image_tag: "latest"

# Argo Workflows - High-Performance Pipeline Execution
argowf_namespace: "argowf"
argowf_nodeport: 32746
argowf_username: "admin"
argowf_password: "mlopsadmin123"
# Enhanced resources for ML pipeline execution
argowf_memory_request: "2Gi"
argowf_memory_limit: "8Gi"
argowf_cpu_request: 1
argowf_cpu_limit: 4

# Kubernetes Dashboard
dashboard_nodeport: 30444

# =============================================================================
# ELASTICSEARCH STACK (OPTIONAL - HIGH RESOURCE USAGE)
# =============================================================================

elasticsearch_namespace: "elastic"
elasticsearch_node_count: 2
elasticsearch_memory_request: "8Gi"
elasticsearch_memory_limit: "16Gi"
elasticsearch_cpu_request: "2"
elasticsearch_cpu_limit: "4"
elasticsearch_storage_size: "200Gi"

# =============================================================================
# SECURITY & NETWORKING
# =============================================================================

# MetalLB Load Balancer Configuration
metallb_namespace: "metallb-system"
metallb_version: "v0.14.8"
metallb_ip_pool_name: "homelab-pool"
metallb_ip_range: "192.168.1.200-192.168.1.250"
metallb_advertisement_name: "homelab-l2-advertisement"
metallb_state: "present"
metallb_controller_resources:
  requests:
    cpu: "100m"
    memory: "100Mi"
  limits:
    cpu: "200m"
    memory: "200Mi"
metallb_speaker_resources:
  requests:
    cpu: "100m"
    memory: "100Mi"
  limits:
    cpu: "200m"
    memory: "200Mi"

# Sealed Secrets
sealed_secrets_namespace: "kube-system"
sealed_secrets_controller_name: "sealed-secrets"

# UFW Firewall
configure_ufw: true
ufw_default_policy_incoming: deny
ufw_default_policy_outgoing: allow

# =============================================================================
# FEATURE FLAGS FOR ON-PREMISES OPTIMIZATION
# =============================================================================

# Enable/Disable heavy components based on needs
enable_elasticsearch: false
enable_kubeflow: false
enable_istio: true
enable_advanced_monitoring: true
enable_gpu_support: false
enable_harbor: true

# Development vs Production modes
on_premises_mode: true
development_mode: true
high_availability: false

# Add these Prometheus Pushgateway variables to your existing monitoring section

# Prometheus Stack Configuration
prometheus_stack_namespace: "monitoring"
prometheus_stack_name: "prometheus-stack"
prometheus_stack_chart_ref: "prometheus-community/kube-prometheus-stack"
prometheus_nodeport: 30090
prometheus_storage_class: "{{ global_storage_class }}"
prometheus_storage_size: "50Gi"
prometheus_memory_request: "4Gi"
prometheus_memory_limit: "8Gi"
prometheus_cpu_request: 2
prometheus_cpu_limit: 4

# Prometheus Pushgateway Configuration
pushgateway_nodeport: 32091
pushgateway_memory_request: "128Mi"
pushgateway_memory_limit: "512Mi"
pushgateway_cpu_request: "100m"
pushgateway_cpu_limit: "500m"

# Helm Configuration
helm_wait_timeout: 600s
helm_retries: 3
helm_retry_delay: 10

# Istio Configuration
istio_gateway_http_nodeport: 31080
istio_gateway_https_nodeport: 31443
istio_gateway_loadbalancer_ip: ""  # Leave empty for auto-assignment from MetalLB pool
istio_pilot_memory_request: "128Mi"
istio_pilot_memory_limit: "512Mi" 
istio_pilot_cpu_request: "100m"
istio_pilot_cpu_limit: "500m"
istio_gateway_memory_request: "128Mi"
istio_gateway_memory_limit: "512Mi"
istio_gateway_cpu_request: "100m"
istio_gateway_cpu_limit: "500m"

# KServe Configuration - Fix namespace to match actual installation
kserve_namespace: "kserve"  # Changed from "kserve-system" to "kserve"
kserve_version: "0.15.0"
knative_version: "1.15.0"
kserve_controller_memory_request: "256Mi"
kserve_controller_memory_limit: "1Gi"
kserve_controller_cpu_request: "100m"
kserve_controller_cpu_limit: "1000m"

# Cert Manager Configuration
certmanager_memory_request: "128Mi"
certmanager_memory_limit: "512Mi"
certmanager_cpu_request: "100m"
certmanager_cpu_limit: "500m"

# Node affinity and scheduling configuration
enable_worker_node_scheduling: true
prefer_worker_nodes_for_jobs: true
control_plane_scheduling_weight: 50  # Lower weight = less preferred
worker_node_scheduling_weight: 100   # Higher weight = more preferred

# PostgreSQL Configuration
postgresql_enabled: true

# PostgreSQL Resources
postgresql_storage_size: 20Gi
postgresql_storage_class: "{{ global_storage_class }}"
postgresql_memory_request: 512Mi
postgresql_memory_limit: 1Gi
postgresql_cpu_request: 250m
postgresql_cpu_limit: 500m

# Harbor Registry Configuration
harbor_namespace: "harbor"
harbor_nodeport: 30880
harbor_admin_password: "Harbor12345"
harbor_secret_key: "not-a-secure-key"
harbor_database_internal_password: "changeit"
harbor_storage_class: "{{ global_storage_class }}"
harbor_storage_size: "50Gi"
harbor_registry_storage_size: "200Gi"
harbor_loadbalancer_enabled: true
harbor_loadbalancer_ip: "192.168.1.210"
harbor_integrate_with_seldon: true
harbor_integrate_with_jupyter: true
harbor_trivy_enabled: true
harbor_notary_enabled: true
harbor_chartmuseum_enabled: true
